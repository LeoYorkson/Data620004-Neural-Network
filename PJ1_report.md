# <center>PJ1 report
#### <center>Liao Yuesen 24210980126

---
### 一、实验目的
本PJ要求从零开始构建三层神经网络分类器，在数据集 CIFAR-10 上进行训练以实现图像分类。利用numpy等库搭建神经网络，并实现前向传播和反向传播算法以及SGD 优化器、学习率下降、交叉熵损失和 L2 正则化等组件。
### 二、实验设置

1.实验环境：
- python 3.10.4
- numpy 1.23.5
- matplotlib 3.7.1
    
2.网络结构：
本实验构建的三层神经网络包括一个输入层、一个隐藏层和一个输出层：
- 输入层：3072个神经元（对应CIFAR-10数据集的32x32x32个像素的RGB图像）
- 隐藏层：通过网格搜索选择256个神经元，使用ReLU作为激活函数
- 输出层：10个神经元（对应CIFAR-10数据集的10个类别），使用softmax作为激活函数

完整的网络结构如下：
```python
class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, activate_model=relu, activation_derivative=relu_derivative):
        self.params = {
            'W1': np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size),
            'b1': np.zeros(hidden_size),
            'W2': np.random.randn(hidden_size, hidden_size) * np.sqrt(2. / hidden_size),
            'b2': np.zeros(hidden_size),
            'W3': np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size),
            'b3': np.zeros(output_size)
        }
```

3.其它设置：
- 数据集：CIFAR-10
- 损失函数：交叉熵损失
- 正则化：L2正则化
- 优化器：SGD
- 学习率：0.01
- 批量大小：64
- 正则化强度：0.001
- 学习轮数：100
- 学习率衰减：每10轮衰减一次

###  三、实验过程
1. 数据预处理
加载CIFAR-10数据集，并进行数据预处理，包括归一化和标签独热编码，将数据集划分为训练集、验证集和测试集。
CIFAR-10数据集包含60000张32x32的彩色图像，分为10个类别，每个类别6000张图像，将数据集划分为50000张训练图像和10000张测试图像，从训练集中随机抽取10000张图像作为验证集，用于调整超参数和早期停止防止过拟合。
对所有图像进行归一化处理，将像素值缩放到[0, 1]范围之间，并将标签转换为独热编码格式，以便于计算交叉熵损失函数。
随机选择一张图像展示如下：
<img src="output.png">
1. 训练流程
对模型进行初始化后，通过一个简单的SGD优化器进行训练，每次迭代中随机选择一个批次的数据进行前向传播和反向传播，计算损失函数和梯度，并更新模型参数。每轮训练后，计算训练集和验证集的损失和准确率，并在每10轮后进行学习率衰减。训练过程中还实现了早停机制，当验证集的损失在连续5轮中没有下降时，停止训练。

1. 网格搜索超参数
通过网格搜索不同的超参数组合，选择其中最佳的组合。具体包括；
- 隐藏层神经元个数(hidden size): 128，256
- 学习率(learning rate): 0.005, 0.01
- 正则化强度(regularization strength): 0.1, 0.01
- 批量大小(batch size): 32, 64

超参数搜索结果如下表所示：
| hidden size | learning rate |regularization strength | batch size | accuracy |
|-------------|---------------|------------------------|------------|----------|
| 128         | 0.005          | 0.1                 | 32         | 0.3140     |
| 128         | 0.005          | 0.1                 | 64         | 0.3390     |
| 128         | 0.005          | 0.01                 | 32         | 0.4900     |
| 128         | 0.005          | 0.01                 | 64         | 0.4860     |
| 128         | 0.01          | 0.1                 | 32         | 0.3030     |
| 128         | 0.01          | 0.1                 | 64         | 0.3240     |
| 128         | 0.01          | 0.01                 | 32         | 0.4930     |
| 128         | 0.01          | 0.01                 | 64         | 0.4890     |
| 256         | 0.005          | 0.1                 | 32         | 0.3210     |
| 256         | 0.005          | 0.1                 | 64         | 0.3470     |
| 256         | 0.005          | 0.01                 | 32         | 0.4840     |
| 256         | 0.005          | 0.01                 | 64         | 0.4890     |
| 256         | 0.01          | 0.1                 | 32         | 0.2920     |
| 256         | 0.01          | 0.1                 | 64         | 0.3460     |
| 256         | 0.01          | 0.01                 | 32         | 0.4810     |
| 256         | 0.01          | 0.01                 | 64         | 0.4970     |
|512         | 0.005          | 0.1                 | 32         | 0.3230     |
| 512         | 0.005          | 0.1                 | 64         | 0.3430     |
| 512         | 0.005          | 0.01                 | 32         | 0.4960     |
| 512         | 0.005          | 0.01                 | 64         | 0.4990     |
|512         | 0.01          | 0.01                 | 32         | 0.4870     |
| 512         | 0.01          | 0.01                 | 64         | 0.5010     |
| 512         | 0.01          | 0.1                 | 32         | 0.3070     |
| 512         | 0.01          | 0.1                 | 64         | 0.3460     |

选取最优参数组合为{hidden size: 512, learning rate: 0.005, regularization strength: 0.001, batch size: 64}，在训练集上进行训练100轮。

1. 测试
在确定最佳超参数配置后，在训练集和测试集上进⾏训练评估，结果如下：
4.1 loss曲线和准确率曲线
<img src="100_epoch.png">
4.2 模型网络参数可视化
在训练完成后，使用matplotlib可视化模型的参数。以下是网络输入层前10个神经元的权重可视化结果：
<img src="1st_layer_neuron.png">
模式分析：暂时分析不出来啥，看着一片糊的噪声一样

### 四、实验结果
通过网格搜索不同的超参数组合，最终选择了256个隐藏层神经元，以0.01作为学习率，0.001作为正则化强度，64作为批量大小，100作为学习轮数。最终训练好的模型在测试集上达到了约50%的准确率，显示出良好的泛化性能。
### 五、实验总结
通过本实验，深入了解了神经网络的基本工作原理及其实现⽅式。通过手动搭建神经网络并实现前向传播和反向传播，加深了对这些核⼼算法的理解。此外，实验中的超参数调优过程也展⽰了调整这些参数对模型性能的重要影响。最终，模型在CIFAR-10数据集上取得了一定的分类效果，验证了网络设计的有效性。

### 六、附录
1. 代码仓库：
2. 模型权重：

